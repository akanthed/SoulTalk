# SoulTalk AI ðŸ§ ðŸ’¬

**Voice-first emotional AI companion** â€” Talk, and it listens with empathy.

Built for hackathon in 2 days. Prioritizes demo impact over production readiness.

---

## How It Works

1. **You speak** â†’ Microphone captures audio
2. **Voxtral** â†’ Speech-to-text transcription (Mistral STT endpoint with fallback)
3. **Mistral Large** â†’ Generates emotionally intelligent response
4. **ElevenLabs** â†’ Text-to-speech with natural voice
5. **You hear** â†’ AI responds with warmth and empathy

---

## Tech Stack

| Layer     | Tech                        |
| --------- | --------------------------- |
| Frontend  | React (Vite) + TailwindCSS  |
| Backend   | Python FastAPI              |
| STT       | Voxtral (Mistral)           |
| LLM       | Mistral Large               |
| TTS       | ElevenLabs                  |
| Memory    | In-memory session memory + entity recall |

---

## Architecture

### System Architecture (High Level)

```mermaid
flowchart LR
        U[User Voice] --> F[Frontend - React/Vite]
        F -->|POST /api/chat| B[FastAPI Backend]
        B --> STT[Mistral STT - Voxtral]
        B --> EMO[Emotion Detection]
        B --> MEM[Session Memory + Entity Recall]
        B --> LLM[Mistral Large]
        B --> TTS[ElevenLabs TTS]
        TTS --> B
        B -->|transcript + response + audio_base64 + emotion| F
        F --> A[Browser Audio Playback]
```

### Runtime Flow (Single Turn)

```mermaid
sequenceDiagram
        participant User
        participant Frontend
        participant Backend
        participant MistralSTT as Mistral STT
        participant MistralLLM as Mistral LLM
        participant ElevenLabs

        User->>Frontend: Speak in microphone
        Frontend->>Backend: POST /chat (audio, session_id)
        Backend->>MistralSTT: Transcribe audio
        MistralSTT-->>Backend: transcript
        Backend->>Backend: detect emotion + update memory
        Backend->>MistralLLM: Generate empathetic response
        MistralLLM-->>Backend: response text
        Backend->>ElevenLabs: Text-to-speech
        ElevenLabs-->>Backend: MP3 bytes
        Backend-->>Frontend: JSON (transcript, response, emotion, audio_base64)
        Frontend-->>User: Show response + play voice
```

### Demo Visuals

- Add screenshots to `docs/images/` using the filenames below.

![SoulTalk Home](docs/images/home.png)

![SoulTalk Chat](docs/images/chat.png)

![SoulTalk Memory Recall](docs/images/memory-recall.png)

---

## Quick Start

### 1. Backend

```bash
cd backend
pip install -r requirements.txt

# Create either .env or .env.local with your API keys
# (.env.local takes priority)
cp .env.example .env.local
# Edit .env.local and add your keys:
#   MISTRAL_API_KEY=your_key
#   ELEVENLABS_API_KEY=your_key
#   ELEVENLABS_VOICE_ID=your_voice_id

python main.py
```

Backend runs on **http://localhost:8000**

### 2. Frontend

```bash
cd frontend
npm install
npm run dev
```

Frontend runs on **http://localhost:5173**

The frontend proxies API requests to the backend automatically.

---

## API Keys Required

| Service     | Get Key At                                    |
| ----------- | --------------------------------------------- |
| Mistral     | https://console.mistral.ai/                   |
| ElevenLabs  | https://elevenlabs.io/                        |

> **Note**: Use a **premade ElevenLabs voice ID** on free tier (for example `EXAVITQu4vr4xnSDxMaL`).

> **Demo mode**: If upstream APIs fail, the backend degrades gracefully so the UI flow still works.

---

## Security / Git Hygiene

- A root `.gitignore` is included.
- Credential files are ignored (`backend/.env`, `backend/.env.local`, other `.env.*`), while `backend/.env.example` is kept for sharing.

---

## Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app (/session, /chat)
â”‚   â”œâ”€â”€ config.py             # Environment config
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ voxtral_service.py    # Speech-to-text
â”‚       â”œâ”€â”€ mistral_service.py    # LLM response generation
â”‚       â”œâ”€â”€ emotion_service.py    # Emotion detection (sad/anxious/confused/neutral)
â”‚       â”œâ”€â”€ elevenlabs_service.py # Text-to-speech
â”‚       â””â”€â”€ memory_service.py     # Session memory + entity extraction
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx          # Entry point
â”‚   â”‚   â”œâ”€â”€ App.tsx           # Router
â”‚   â”‚   â”œâ”€â”€ api.ts            # Backend API client
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Home.tsx      # Landing page
â”‚   â”‚   â”‚   â””â”€â”€ Chat.tsx      # Chat interface
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ WaveAnimation.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ EmotionBadge.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ThinkingIndicator.tsx
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â””â”€â”€ useAudioRecorder.ts
â”‚   â””â”€â”€ index.html
â””â”€â”€ prompts/
    â””â”€â”€ system_prompt.txt     # AI personality prompt
```

---

## Features

- ðŸŽ¤ Voice input with real-time recording
- ðŸ§  Emotion-aware responses with validation-first style
- ðŸ’¾ Memory recall for people, emotions, and situations
- âœ¨ Demo-ready recall behavior (including dad/stress flow)
- ðŸ”Š ElevenLabs speech output with pause-aware phrasing
- ðŸŒŠ Audio wave + reflective thinking UX
- ðŸŽ¨ Clean dark UI optimized for quick demos

---

## License

Hackathon project â€” use freely.
